"""
åˆ†å¸ƒå¼æ¨ç† KV-Cache å®¹é”™ Demo
æ¨¡æ‹Ÿåœºæ™¯ï¼š4ä¸ªè®¾å¤‡åˆ†åˆ«æŒæœ‰ä¸åŒçš„æ³¨æ„åŠ›å¤´ï¼ŒæŸä¸ªè®¾å¤‡ä¸‹çº¿åå…¶ä»–è®¾å¤‡æ¥ç®¡å¹¶å¤ç”¨ç°æœ‰ KV-Cache
"""

import json
import torch
import torch.nn.functional as F
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import time
from dataclasses import dataclass
from safetensors import safe_open

# ==================== é…ç½®éƒ¨åˆ† ====================
MODEL_PATH = "A"  # æ›¿æ¢ä¸ºä½ çš„ model.safetensors è·¯å¾„
PARAMS_PATH = "B"  # æ›¿æ¢ä¸ºä½ çš„ params.json è·¯å¾„
TOKENIZER_PATH = "C"  # æ›¿æ¢ä¸ºä½ çš„ tokenizer.model è·¯å¾„

NUM_DEVICES = 4  # æ¨¡æ‹Ÿ4ä¸ªè®¾å¤‡
OFFLINE_DEVICE = 1  # æ¨¡æ‹Ÿç¬¬2ä¸ªè®¾å¤‡ä¸‹çº¿ï¼ˆç´¢å¼•ä»0å¼€å§‹ï¼‰
OFFLINE_AT_TOKEN = 5  # åœ¨ç”Ÿæˆç¬¬5ä¸ªtokenåè®¾å¤‡ä¸‹çº¿


# ==================== æ•°æ®ç»“æ„ ====================
@dataclass
class DeviceState:
    """è®¾å¤‡çŠ¶æ€"""
    device_id: int
    head_range: Tuple[int, int]  # (start_head, end_head) å·¦é—­å³å¼€
    is_online: bool = True
    kv_cache: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None  # [(k, v)] for each layer


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    dim: int
    n_layers: int
    n_heads: int
    n_kv_heads: int
    vocab_size: int
    norm_eps: float
    rope_theta: float = 500000.0
    
    @property
    def head_dim(self):
        return self.dim // self.n_heads


# ==================== ç®€åŒ–çš„ Tokenizer ====================
class SimpleTokenizer:
    """ç®€åŒ–çš„ Tokenizerï¼ˆç”¨äº Demoï¼‰"""
    def __init__(self, tokenizer_path: str):
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åŠ è½½ sentencepiece
        self.bos_id = 128000
        self.eos_id = 128001
        
    def encode(self, text: str) -> List[int]:
        """ç®€åŒ–ç¼–ç ï¼šä¸º Demo ç”Ÿæˆå‡çš„ token ids"""
        # å®é™…ä½¿ç”¨æ—¶åº”è¯¥ç”¨ sentencepiece æˆ– transformers
        return [self.bos_id] + [ord(c) % 1000 + 100 for c in text]
    
    def decode(self, tokens: List[int]) -> str:
        """ç®€åŒ–è§£ç """
        return f"<generated_{len(tokens)}_tokens>"


# ==================== æ¨¡å‹åŠ è½½ ====================
def load_model_config(params_path: str) -> ModelConfig:
    """åŠ è½½æ¨¡å‹é…ç½®"""
    with open(params_path, 'r') as f:
        params = json.load(f)
    
    return ModelConfig(
        dim=params['dim'],
        n_layers=params['n_layers'],
        n_heads=params['n_heads'],
        n_kv_heads=params.get('n_kv_heads', params['n_heads']),
        vocab_size=params['vocab_size'],
        norm_eps=params['norm_eps'],
        rope_theta=params.get('rope_theta', 500000.0)
    )


def load_model_weights(model_path: str) -> Dict[str, torch.Tensor]:
    """åŠ è½½æ¨¡å‹æƒé‡"""
    weights = {}
    with safe_open(model_path, framework="pt", device="cpu") as f:
        for key in f.keys():
            weights[key] = f.get_tensor(key)
    return weights


# ==================== RoPE å®ç° ====================
def precompute_freqs_cis(dim: int, end: int, theta: float = 500000.0):
    """é¢„è®¡ç®— RoPE çš„é¢‘ç‡"""
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis


def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):
    """åº”ç”¨ RoPE"""
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    
    freqs_cis = freqs_cis[:xq_.shape[1]]
    
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)
    
    return xq_out.type_as(xq), xk_out.type_as(xk)


# ==================== æ³¨æ„åŠ›è®¡ç®—ï¼ˆæ”¯æŒéƒ¨åˆ†å¤´ï¼‰ ====================
def compute_attention_partial_heads(
    x: torch.Tensor,
    layer_idx: int,
    weights: Dict[str, torch.Tensor],
    config: ModelConfig,
    head_range: Tuple[int, int],
    kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]],
    freqs_cis: torch.Tensor,
    positions: torch.Tensor
) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
    """
    è®¡ç®—éƒ¨åˆ†æ³¨æ„åŠ›å¤´
    
    Args:
        x: è¾“å…¥ [batch, seq_len, dim]
        layer_idx: å±‚ç´¢å¼•
        weights: æ¨¡å‹æƒé‡
        config: æ¨¡å‹é…ç½®
        head_range: (start_head, end_head) è¦è®¡ç®—çš„å¤´èŒƒå›´
        kv_cache: å·²æœ‰çš„ KV-Cache (k, v)
        freqs_cis: RoPE é¢‘ç‡
        positions: ä½ç½®ç´¢å¼•
    
    Returns:
        attention_output: [batch, seq_len, head_dim * num_heads_in_range]
        new_kv_cache: æ›´æ–°åçš„ (k, v)
    """
    batch, seq_len, dim = x.shape
    start_head, end_head = head_range
    num_heads_in_range = end_head - start_head
    head_dim = config.head_dim
    
    # åŠ è½½æƒé‡
    wq = weights[f'layers.{layer_idx}.attention.wq.weight']
    wk = weights[f'layers.{layer_idx}.attention.wk.weight']
    wv = weights[f'layers.{layer_idx}.attention.wv.weight']
    
    # åªå–å¯¹åº”å¤´çš„æƒé‡åˆ‡ç‰‡
    head_start_dim = start_head * head_dim
    head_end_dim = end_head * head_dim
    
    wq_slice = wq[head_start_dim:head_end_dim, :]
    wk_slice = wk[head_start_dim:head_end_dim, :]
    wv_slice = wv[head_start_dim:head_end_dim, :]
    
    # è®¡ç®— Q, K, V
    q = F.linear(x, wq_slice)  # [batch, seq_len, num_heads_in_range * head_dim]
    k = F.linear(x, wk_slice)
    v = F.linear(x, wv_slice)
    
    # Reshape
    q = q.view(batch, seq_len, num_heads_in_range, head_dim)
    k = k.view(batch, seq_len, num_heads_in_range, head_dim)
    v = v.view(batch, seq_len, num_heads_in_range, head_dim)
    
    # åº”ç”¨ RoPE
    q, k = apply_rotary_emb(q, k, freqs_cis[positions])
    
    # æ›´æ–° KV-Cache
    if kv_cache is not None:
        k_cache, v_cache = kv_cache
        k = torch.cat([k_cache, k], dim=1)
        v = torch.cat([v_cache, v], dim=1)
    
    # è®¡ç®—æ³¨æ„åŠ›
    q = q.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]
    k = k.transpose(1, 2)
    v = v.transpose(1, 2)
    
    scores = torch.matmul(q, k.transpose(2, 3)) / (head_dim ** 0.5)
    scores = F.softmax(scores, dim=-1)
    output = torch.matmul(scores, v)  # [batch, num_heads, seq_len, head_dim]
    
    # Reshape
    output = output.transpose(1, 2).contiguous().view(batch, seq_len, -1)
    
    return output, (k.transpose(1, 2), v.transpose(1, 2))


# ==================== è®¾å¤‡ç®¡ç† ====================
def initialize_devices(config: ModelConfig, num_devices: int) -> List[DeviceState]:
    """åˆå§‹åŒ–è®¾å¤‡ï¼Œåˆ†é…æ³¨æ„åŠ›å¤´"""
    heads_per_device = config.n_heads // num_devices
    devices = []
    
    for i in range(num_devices):
        start_head = i * heads_per_device
        end_head = (i + 1) * heads_per_device if i < num_devices - 1 else config.n_heads
        
        device = DeviceState(
            device_id=i,
            head_range=(start_head, end_head),
            kv_cache=[None] * config.n_layers  # æ¯å±‚ä¸€ä¸ª KV-Cache
        )
        devices.append(device)
        print(f"è®¾å¤‡ {i}: è´Ÿè´£å¤´ [{start_head}, {end_head})")
    
    return devices


def reassign_heads_after_failure(
    devices: List[DeviceState],
    offline_device_id: int,
    config: ModelConfig
) -> Dict[int, List[Tuple[int, int]]]:
    """
    è®¾å¤‡ä¸‹çº¿åé‡æ–°åˆ†é…å¤´
    
    è¿”å›: {device_id: [(start_head, end_head), ...]} é¢å¤–åˆ†é…çš„å¤´
    """
    offline_device = devices[offline_device_id]
    offline_heads = offline_device.head_range
    
    print(f"\nâš ï¸  è®¾å¤‡ {offline_device_id} ä¸‹çº¿ï¼Œä¸¢å¤±å¤´ {offline_heads}")
    
    # ç®€å•ç­–ç•¥ï¼šå°†ä¸¢å¤±çš„å¤´å¹³å‡åˆ†é…ç»™å…¶ä»–åœ¨çº¿è®¾å¤‡
    online_devices = [d for d in devices if d.is_online and d.device_id != offline_device_id]
    num_lost_heads = offline_heads[1] - offline_heads[0]
    heads_per_online_device = num_lost_heads // len(online_devices)
    
    reassignment = {}
    current_head = offline_heads[0]
    
    for i, device in enumerate(online_devices):
        extra_heads = heads_per_online_device
        if i == len(online_devices) - 1:  # æœ€åä¸€ä¸ªè®¾å¤‡æ‰¿æ‹…å‰©ä½™çš„
            extra_heads = offline_heads[1] - current_head
        
        reassignment[device.device_id] = [(current_head, current_head + extra_heads)]
        print(f"  â†’ è®¾å¤‡ {device.device_id} é¢å¤–æ‰¿æ‹…å¤´ [{current_head}, {current_head + extra_heads})")
        current_head += extra_heads
    
    return reassignment


# ==================== æ¨ç†ä¸»æµç¨‹ ====================
def generate_token(
    x: torch.Tensor,
    devices: List[DeviceState],
    weights: Dict[str, torch.Tensor],
    config: ModelConfig,
    freqs_cis: torch.Tensor,
    positions: torch.Tensor,
    extra_assignments: Optional[Dict[int, List[Tuple[int, int]]]] = None
) -> torch.Tensor:
    """
    ç”Ÿæˆä¸€ä¸ª token
    
    Args:
        x: è¾“å…¥ embeddings [batch, seq_len, dim]
        devices: è®¾å¤‡åˆ—è¡¨
        weights: æ¨¡å‹æƒé‡
        config: æ¨¡å‹é…ç½®
        freqs_cis: RoPE é¢‘ç‡
        positions: ä½ç½®ç´¢å¼•
        extra_assignments: è®¾å¤‡ä¸‹çº¿åçš„é¢å¤–å¤´åˆ†é…
    
    Returns:
        logits: [batch, seq_len, vocab_size]
    """
    batch, seq_len, dim = x.shape
    
    # é€å±‚è®¡ç®—
    for layer_idx in range(config.n_layers):
        # RMSNorm (ç®€åŒ–ï¼šç›´æ¥ç”¨ layer_norm)
        attn_input = x
        
        # æ”¶é›†æ‰€æœ‰è®¾å¤‡çš„æ³¨æ„åŠ›è¾“å‡º
        all_head_outputs = [None] * config.n_heads
        
        for device in devices:
            if not device.is_online:
                continue
            
            # è®¡ç®—è®¾å¤‡åŸæœ¬è´Ÿè´£çš„å¤´
            head_ranges = [device.head_range]
            
            # å¦‚æœæœ‰é¢å¤–åˆ†é…çš„å¤´ï¼ˆå› ä¸ºå…¶ä»–è®¾å¤‡ä¸‹çº¿ï¼‰
            if extra_assignments and device.device_id in extra_assignments:
                head_ranges.extend(extra_assignments[device.device_id])
            
            for head_range in head_ranges:
                start_head, end_head = head_range
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯å¤ç”¨çš„ KV-Cache
                kv_cache = device.kv_cache[layer_idx]
                
                # å¦‚æœæ˜¯é¢å¤–åˆ†é…çš„å¤´ï¼ˆæ¥è‡ªä¸‹çº¿è®¾å¤‡ï¼‰ï¼Œéœ€è¦é‡æ–°è®¡ç®—
                if head_range != device.head_range:
                    print(f"  ğŸ”„ è®¾å¤‡ {device.device_id} é‡æ–°è®¡ç®—å¤´ [{start_head}, {end_head}) (Layer {layer_idx})")
                    kv_cache = None  # å¼ºåˆ¶é‡æ–°è®¡ç®—
                else:
                    if kv_cache is not None:
                        print(f"  âœ… è®¾å¤‡ {device.device_id} å¤ç”¨ KV-Cache å¤´ [{start_head}, {end_head}) (Layer {layer_idx})")
                
                # è®¡ç®—æ³¨æ„åŠ›
                attn_out, new_kv = compute_attention_partial_heads(
                    attn_input,
                    layer_idx,
                    weights,
                    config,
                    head_range,
                    kv_cache,
                    freqs_cis,
                    positions
                )
                
                # æ›´æ–° KV-Cacheï¼ˆå¦‚æœæ˜¯åŸè®¾å¤‡çš„å¤´ï¼‰
                if head_range == device.head_range:
                    device.kv_cache[layer_idx] = new_kv
                
                # å­˜å‚¨è¾“å‡ºåˆ°å¯¹åº”çš„å¤´ä½ç½®
                num_heads_in_range = end_head - start_head
                head_dim = config.head_dim
                attn_out_reshaped = attn_out.view(batch, seq_len, num_heads_in_range, head_dim)
                
                for i, head_idx in enumerate(range(start_head, end_head)):
                    all_head_outputs[head_idx] = attn_out_reshaped[:, :, i, :]
        
        # æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
        concat_heads = torch.cat(all_head_outputs, dim=-1)  # [batch, seq_len, n_heads * head_dim]
        concat_heads = concat_heads.view(batch, seq_len, dim)
        
        # è¾“å‡ºæŠ•å½±
        wo = weights[f'layers.{layer_idx}.attention.wo.weight']
        attn_output = F.linear(concat_heads, wo)
        
        # æ®‹å·®è¿æ¥
        x = attn_input + attn_output
        
        # FFN (ç®€åŒ–ï¼šè·³è¿‡ï¼Œåªæ¼”ç¤ºæ³¨æ„åŠ›éƒ¨åˆ†)
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦å®Œæ•´å®ç°
    
    # è¾“å‡ºå±‚ (ç®€åŒ–)
    norm_weight = weights['norm.weight']
    x = F.layer_norm(x, (dim,), norm_weight)
    
    output_weight = weights['output.weight']
    logits = F.linear(x, output_weight)
    
    return logits


# ==================== ä¸»å‡½æ•° ====================
def main():
    print("=" * 60)
    print("åˆ†å¸ƒå¼æ¨ç† KV-Cache å®¹é”™ Demo")
    print("=" * 60)
    
    # 1. åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    config = load_model_config(PARAMS_PATH)
    weights = load_model_weights(MODEL_PATH)
    tokenizer = SimpleTokenizer(TOKENIZER_PATH)
    
    print(f"  æ¨¡å‹: Llama-3.2-1B")
    print(f"  å±‚æ•°: {config.n_layers}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {config.n_heads}")
    print(f"  å¤´ç»´åº¦: {config.head_dim}")
    
    # 2. åˆå§‹åŒ–è®¾å¤‡
    print(f"\nğŸ–¥ï¸  åˆå§‹åŒ– {NUM_DEVICES} ä¸ªè®¾å¤‡...")
    devices = initialize_devices(config, NUM_DEVICES)
    
    # 3. å‡†å¤‡è¾“å…¥
    print("\nğŸ“ å‡†å¤‡è¾“å…¥...")
    prompt = "Hello"
    tokens = tokenizer.encode(prompt)[:10]  # é™åˆ¶é•¿åº¦
    print(f"  è¾“å…¥ tokens: {tokens[:5]}...")
    
    # é¢„è®¡ç®— RoPE
    max_seq_len = 128
    freqs_cis = precompute_freqs_cis(config.head_dim, max_seq_len, config.rope_theta)
    
    # 4. å¼€å§‹ç”Ÿæˆ
    print("\nğŸš€ å¼€å§‹ç”Ÿæˆ...")
    num_tokens_to_generate = 10
    extra_assignments = None
    
    for step in range(num_tokens_to_generate):
        print(f"\n{'='*60}")
        print(f"ç”Ÿæˆç¬¬ {step + 1} ä¸ª Token")
        print(f"{'='*60}")
        
        # æ¨¡æ‹Ÿè®¾å¤‡ä¸‹çº¿
        if step == OFFLINE_AT_TOKEN:
            devices[OFFLINE_DEVICE].is_online = False
            extra_assignments = reassign_heads_after_failure(devices, OFFLINE_DEVICE, config)
        
        # å‡†å¤‡è¾“å…¥
        if step == 0:
            input_ids = torch.tensor([tokens], dtype=torch.long)
        else:
            input_ids = torch.tensor([[tokens[-1]]], dtype=torch.long)
        
        # Embedding
        embed_weight = weights['tok_embeddings.weight']
        x = F.embedding(input_ids, embed_weight)
        
        # ä½ç½®ä¿¡æ¯
        if step == 0:
            positions = torch.arange(len(tokens), dtype=torch.long)
        else:
            positions = torch.tensor([len(tokens) - 1], dtype=torch.long)
        
        # ç”Ÿæˆ (åªæ¼”ç¤ºç¬¬ä¸€å±‚)
        print("\nå¤„ç†ç¬¬ 0 å±‚:")
        start_time = time.time()
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåªè®¡ç®—ä¸€å±‚æ¥æ¼”ç¤º
        layer_idx = 0
        attn_input = x
        all_head_outputs = [None] * config.n_heads
        
        for device in devices:
            if not device.is_online:
                continue
            
            head_ranges = [device.head_range]
            if extra_assignments and device.device_id in extra_assignments:
                head_ranges.extend(extra_assignments[device.device_id])
            
            for head_range in head_ranges:
                start_head, end_head = head_range
                kv_cache = device.kv_cache[layer_idx]
                
                if head_range != device.head_range:
                    print(f"  ğŸ”„ è®¾å¤‡ {device.device_id} é‡æ–°è®¡ç®—å¤´ [{start_head}, {end_head})")
                    # éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—
                    if step > 0:
                        # é‡æ–°è®¡ç®—ä¹‹å‰çš„æ‰€æœ‰ tokens
                        full_input_ids = torch.tensor([tokens], dtype=torch.long)
                        full_x = F.embedding(full_input_ids, embed_weight)
                        full_positions = torch.arange(len(tokens), dtype=torch.long)
                        
                        attn_out, new_kv = compute_attention_partial_heads(
                            full_x,
                            layer_idx,
                            weights,
                            config,
                            head_range,
                            None,  # ä»å¤´è®¡ç®—
                            freqs_cis,
                            full_positions
                        )
                        # åªå–æœ€åä¸€ä¸ª token çš„è¾“å‡º
                        attn_out = attn_out[:, -1:, :]
                    else:
                        attn_out, new_kv = compute_attention_partial_heads(
                            attn_input,
                            layer_idx,
                            weights,
                            config,
                            head_range,
                            None,
                            freqs_cis,
                            positions
                        )
                else:
                    if kv_cache is not None:
                        print(f"  âœ… è®¾å¤‡ {device.device_id} å¤ç”¨ KV-Cache å¤´ [{start_head}, {end_head})")
                    
                    attn_out, new_kv = compute_attention_partial_heads(
                        attn_input,
                        layer_idx,
                        weights,
                        config,
                        head_range,
                        kv_cache,
                        freqs_cis,
                        positions
                    )
                    
                    if head_range == device.head_range:
                        device.kv_cache[layer_idx] = new_kv
                
                # å­˜å‚¨è¾“å‡º
                num_heads_in_range = end_head - start_head
                head_dim = config.head_dim
                attn_out_reshaped = attn_out.view(1, -1, num_heads_in_range, head_dim)
                
                for i, head_idx in enumerate(range(start_head, end_head)):
                    all_head_outputs[head_idx] = attn_out_reshaped[:, :, i, :]
        
        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  è€—æ—¶: {elapsed:.4f}s")
        
        # ç®€åŒ–ï¼šç›´æ¥ç”Ÿæˆå‡çš„ä¸‹ä¸€ä¸ª token
        next_token = 100 + step
        tokens.append(next_token)
        
        print(f"âœ… ç”Ÿæˆ token: {next_token}")
    
    print("\n" + "=" * 60)
    print("âœ… Demo å®Œæˆï¼")
    print("=" * 60)
    print(f"\næ€»ç»“:")
    print(f"  - ç”Ÿæˆäº† {num_tokens_to_generate} ä¸ª tokens")
    print(f"  - åœ¨ç¬¬ {OFFLINE_AT_TOKEN + 1} ä¸ª token æ—¶è®¾å¤‡ {OFFLINE_DEVICE} ä¸‹çº¿")
    print(f"  - æˆåŠŸå¤ç”¨äº†å…¶ä»–è®¾å¤‡çš„ KV-Cache")
    print(f"  - ä»…å¯¹ä¸¢å¤±éƒ¨åˆ†è¿›è¡Œäº†é‡æ–°è®¡ç®—")


if __name__ == "__main__":
    main()